{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "30bed346",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pygame in ./PYTHON/nven/lib/python3.10/site-packages (2.6.1)\n"
     ]
    }
   ],
   "source": [
    "!pip3 install pygame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "776f40fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "  MAZE RUNNER: Q-LEARNING PROJECT\n",
      "======================================================================\n",
      "\n",
      " Units: MDP | Bellman | Q-Learning | Tabular Methods\n",
      "======================================================================\n",
      "\n",
      "Creating Environment...\n",
      "Initializing Q-Learning Agent...\n",
      "Starting Training...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "import pygame\n",
    "from collections import deque\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "\n",
    "\n",
    "class MazeEnv(gym.Env):\n",
    "    metadata = {'render_modes': ['human', 'rgb_array']}\n",
    "    # 1. MDP Environment for Maze Runner\n",
    "    def __init__(self, maze_size=10, render_mode=None):\n",
    "        super().__init__()\n",
    "        self.size = maze_size\n",
    "        self.render_mode = render_mode\n",
    "        self.action_space = spaces.Discrete(4)\n",
    "        self.observation_space = spaces.Discrete(self.size * self.size)\n",
    "        self.actions = {0: (-1, 0), 1: (1, 0), 2: (0, -1), 3: (0, 1)}  # UP, DOWN, LEFT, RIGHT\n",
    "        self.maze = self._create_maze()\n",
    "        self.start_pos = (0, 0)\n",
    "        self.goal_pos = (self.size - 1, self.size - 1)\n",
    "        self.agent_pos = self.start_pos\n",
    "        self.cell_size = 60\n",
    "\n",
    "    def _create_maze(self):\n",
    "        maze = np.zeros((self.size, self.size), dtype=int)\n",
    "        if self.size >= 10:\n",
    "            maze[2:7, 3] = 1\n",
    "            maze[4:9, 6] = 1\n",
    "            maze[3, 5:8] = 1\n",
    "            maze[6, 1:4] = 1\n",
    "            maze[4, 4] = 2  # traps\n",
    "            maze[7, 7] = 2\n",
    "            maze[8,1]=2 \n",
    "        maze[0, 0] = 0\n",
    "        maze[self.size - 1, self.size - 1] = 0\n",
    "        return maze\n",
    "\n",
    "    def _pos_to_state(self, pos):\n",
    "        return pos[0] * self.size + pos[1]\n",
    "\n",
    "    def reset(self, seed=None, options=None):\n",
    "        super().reset(seed=seed)\n",
    "        self.agent_pos = self.start_pos\n",
    "        return self._pos_to_state(self.agent_pos), {}\n",
    "\n",
    "    # 3. TRANSITION FUNCTION P(s'|s,a)\n",
    "    def step(self, action):\n",
    "        move = self.actions[action]\n",
    "        new_pos = (self.agent_pos[0] + move[0], self.agent_pos[1] + move[1])\n",
    "        terminated = False\n",
    "\n",
    "        if (new_pos[0] < 0 or new_pos[0] >= self.size or\n",
    "                new_pos[1] < 0 or new_pos[1] >= self.size):\n",
    "            reward = -10\n",
    "            new_pos = self.agent_pos\n",
    "\n",
    "        elif self.maze[new_pos] == 1:\n",
    "            reward = -10\n",
    "            new_pos = self.agent_pos\n",
    "\n",
    "        elif self.maze[new_pos] == 2:\n",
    "            reward = -50\n",
    "            self.agent_pos = new_pos\n",
    "            terminated = True  # Fell into trap (pothole)\n",
    "\n",
    "        elif new_pos == self.goal_pos:\n",
    "            reward = 100\n",
    "            self.agent_pos = new_pos\n",
    "            terminated = True  # Goal reached\n",
    "\n",
    "        else:\n",
    "            reward = -0.1\n",
    "            self.agent_pos = new_pos\n",
    "\n",
    "        # Keep trap termination AND goal termination\n",
    "        terminated = terminated or (self.agent_pos == self.goal_pos)\n",
    "\n",
    "        # Return info about whether it fell into trap\n",
    "        return self._pos_to_state(self.agent_pos), reward, terminated, False, {\"fell_in_trap\": self.maze[new_pos] == 2}\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------------\n",
    "# Q-Learning Agent\n",
    "# ---------------------------------------------------------------\n",
    "class QLearningAgent:\n",
    "    def __init__(self, state_size, action_size, learning_rate=0.1,\n",
    "                 discount_factor=0.99, epsilon=1.0, epsilon_decay=0.995, epsilon_min=0.01):\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.alpha = learning_rate\n",
    "        self.gamma = discount_factor\n",
    "        self.epsilon = epsilon\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        self.epsilon_min = epsilon_min\n",
    "        self.q_table = np.zeros((state_size, action_size))\n",
    "        self.training_info = {\n",
    "            'episodes': [], 'rewards': [], 'steps': [],\n",
    "            'epsilons': [], 'avg_q_values': []\n",
    "        }\n",
    "\n",
    "    def get_action(self, state, training=True):\n",
    "        if training and np.random.rand() < self.epsilon:\n",
    "            return np.random.randint(self.action_size)\n",
    "        return np.argmax(self.q_table[state])\n",
    "\n",
    "    def update(self, state, action, reward, next_state, done):\n",
    "        current_q = self.q_table[state, action]\n",
    "        target_q = reward if done else reward + self.gamma * np.max(self.q_table[next_state])\n",
    "        self.q_table[state, action] += self.alpha * (target_q - current_q)\n",
    "\n",
    "    def decay_epsilon(self):\n",
    "        self.epsilon = max(self.epsilon_min, self.epsilon * self.epsilon_decay)\n",
    "\n",
    "    def get_policy(self):\n",
    "        return np.argmax(self.q_table, axis=1)\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------------\n",
    "# Visualizer\n",
    "# ---------------------------------------------------------------\n",
    "class EpisodeVisualizer:\n",
    "    def __init__(self, env, agent):\n",
    "        pygame.init()\n",
    "        self.env = env\n",
    "        self.agent = agent\n",
    "        self.maze_width = env.size * env.cell_size\n",
    "        self.info_width = 450\n",
    "        self.total_width = self.maze_width + self.info_width\n",
    "        self.total_height = env.size * env.cell_size\n",
    "        self.screen = pygame.display.set_mode((self.total_width, self.total_height))\n",
    "        pygame.display.set_caption(\"Maze Runner Q-Learning\")\n",
    "        self.clock = pygame.time.Clock()\n",
    "        self.font = pygame.font.Font(None, 26)\n",
    "        self.small_font = pygame.font.Font(None, 20)\n",
    "        self.reward_history = deque(maxlen=100)\n",
    "        self.steps_history = deque(maxlen=100)\n",
    "\n",
    "    def draw_episode(self, trajectory, episode, total_episodes, steps, total_reward, epsilon, success, fell_in_trap=False):\n",
    "        for event in pygame.event.get():\n",
    "            if event.type == pygame.QUIT:\n",
    "                return False\n",
    "\n",
    "        WHITE, BLACK = (255, 255, 255), (40, 40, 40)\n",
    "        WALL, TRAP = (60, 60, 60), (200, 50, 50)\n",
    "        AGENT, GOAL = (50, 150, 250), (50, 200, 50)\n",
    "        PATH, TRAJECTORY = (240, 240, 240), (100, 200, 255)\n",
    "\n",
    "        self.screen.fill(WHITE)\n",
    "\n",
    "        for i in range(self.env.size):\n",
    "            for j in range(self.env.size):\n",
    "                x, y = j * self.env.cell_size, i * self.env.cell_size\n",
    "                if self.env.maze[i, j] == 1:\n",
    "                    pygame.draw.rect(self.screen, WALL, (x, y, self.env.cell_size, self.env.cell_size))\n",
    "                elif self.env.maze[i, j] == 2:\n",
    "                    pygame.draw.rect(self.screen, TRAP, (x, y, self.env.cell_size, self.env.cell_size))\n",
    "                else:\n",
    "                    pygame.draw.rect(self.screen, PATH, (x, y, self.env.cell_size, self.env.cell_size))\n",
    "                pygame.draw.rect(self.screen, BLACK, (x, y, self.env.cell_size, self.env.cell_size), 1)\n",
    "\n",
    "        if len(trajectory) > 1:\n",
    "            for i in range(len(trajectory) - 1):\n",
    "                pos1, pos2 = trajectory[i], trajectory[i + 1]\n",
    "                x1 = pos1[1] * self.env.cell_size + self.env.cell_size // 2\n",
    "                y1 = pos1[0] * self.env.cell_size + self.env.cell_size // 2\n",
    "                x2 = pos2[1] * self.env.cell_size + self.env.cell_size // 2\n",
    "                y2 = pos2[0] * self.env.cell_size + self.env.cell_size // 2\n",
    "                pygame.draw.line(self.screen, TRAJECTORY, (x1, y1), (x2, y2), 3)\n",
    "\n",
    "        gx = self.env.goal_pos[1] * self.env.cell_size\n",
    "        gy = self.env.goal_pos[0] * self.env.cell_size\n",
    "        pygame.draw.rect(self.screen, GOAL, (gx + 5, gy + 5, self.env.cell_size - 10, self.env.cell_size - 10))\n",
    "\n",
    "        sx = self.env.start_pos[1] * self.env.cell_size + self.env.cell_size // 2\n",
    "        sy = self.env.start_pos[0] * self.env.cell_size + self.env.cell_size // 2\n",
    "        pygame.draw.circle(self.screen, (255, 0, 0), (sx, sy), 8)\n",
    "\n",
    "        if trajectory:\n",
    "            final_pos = trajectory[-1]\n",
    "            ax = final_pos[1] * self.env.cell_size + self.env.cell_size // 2\n",
    "            ay = final_pos[0] * self.env.cell_size + self.env.cell_size // 2\n",
    "            pygame.draw.circle(self.screen, AGENT, (ax, ay), self.env.cell_size // 3)\n",
    "\n",
    "        self._draw_info_panel(episode, total_episodes, steps, total_reward, epsilon, success, fell_in_trap)\n",
    "        pygame.display.flip()\n",
    "        self.clock.tick(10)\n",
    "        return True\n",
    "\n",
    "    def _draw_info_panel(self, episode, total_episodes, steps, total_reward, epsilon, success, fell_in_trap=False):\n",
    "        WHITE, BLACK = (255, 255, 255), (0, 0, 0)\n",
    "        GRAY, BLUE = (200, 200, 200), (50, 150, 250)\n",
    "        GREEN, RED = (50, 200, 50), (200, 50, 50)\n",
    "\n",
    "        panel_x = self.maze_width\n",
    "        pygame.draw.rect(self.screen, WHITE, (panel_x, 0, self.info_width, self.total_height))\n",
    "        pygame.draw.line(self.screen, BLACK, (panel_x, 0), (panel_x, self.total_height), 2)\n",
    "\n",
    "        y = 20\n",
    "        title = self.font.render(\"Q-Learning Training\", True, BLUE)\n",
    "        self.screen.blit(title, (panel_x + 20, y))\n",
    "        y += 50\n",
    "\n",
    "        progress = episode / total_episodes\n",
    "        bar_w, bar_h = self.info_width - 40, 25\n",
    "        pygame.draw.rect(self.screen, GRAY, (panel_x + 20, y, bar_w, bar_h))\n",
    "        pygame.draw.rect(self.screen, GREEN, (panel_x + 20, y, int(bar_w * progress), bar_h))\n",
    "        prog_text = self.small_font.render(f\"{episode}/{total_episodes} ({progress*100:.1f}%)\", True, BLACK)\n",
    "        self.screen.blit(prog_text, (panel_x + 20, y + 30))\n",
    "        y += 70\n",
    "\n",
    "        # Status message logic\n",
    "        if success:\n",
    "            status_text = \"SUCCESS! ðŸŽ¯\"\n",
    "            status_color = GREEN\n",
    "        elif fell_in_trap:\n",
    "            status_text = \"Fell into Pothole! ðŸ’€\"\n",
    "            status_color = (255, 80, 80)\n",
    "        else:\n",
    "            status_text = \"Failed/Timeout\"\n",
    "            status_color = RED\n",
    "\n",
    "        for text, color in [\n",
    "            (f\"Episode: {episode}\", BLACK),\n",
    "            (f\"Status: {status_text}\", status_color),\n",
    "            (f\"Steps: {steps}\", BLACK),\n",
    "            (f\"Reward: {total_reward:.2f}\", BLACK),\n",
    "            (f\"Epsilon: {epsilon:.3f}\", BLACK),\n",
    "            (\"\", BLACK),\n",
    "            (\"Recent Performance:\", BLUE)\n",
    "        ]:\n",
    "            surf = self.small_font.render(text, True, color)\n",
    "            self.screen.blit(surf, (panel_x + 20, y))\n",
    "            y += 28\n",
    "\n",
    "        if len(self.reward_history) > 0:\n",
    "            avg_r = np.mean(self.reward_history)\n",
    "            avg_s = np.mean(self.steps_history)\n",
    "            succ_rate = sum(1 for r in self.reward_history if r > 50) / len(self.reward_history) * 100\n",
    "\n",
    "            for text in [\n",
    "                f\"Avg Reward (100): {avg_r:.2f}\",\n",
    "                f\"Avg Steps (100): {avg_s:.1f}\",\n",
    "                f\"Success Rate: {succ_rate:.1f}%\"\n",
    "            ]:\n",
    "                surf = self.small_font.render(text, True, BLACK)\n",
    "                self.screen.blit(surf, (panel_x + 20, y))\n",
    "                y += 25\n",
    "\n",
    "    def add_episode_data(self, reward, steps):\n",
    "        self.reward_history.append(reward)\n",
    "        self.steps_history.append(steps)\n",
    "\n",
    "    def close(self):\n",
    "        pygame.quit()\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------------\n",
    "# Training\n",
    "# ---------------------------------------------------------------\n",
    "def train_agent(env, agent, num_episodes=500, visualize=True, show_every=1):\n",
    "    print(\"=\" * 60)\n",
    "    print(\"MAZE RUNNER Q-LEARNING TRAINING\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"Episodes: {num_episodes} | Visualization: Every {show_every} episode(s)\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    visualizer = EpisodeVisualizer(env, agent) if visualize else None\n",
    "\n",
    "    for episode in range(num_episodes):\n",
    "        state, _ = env.reset()\n",
    "        total_reward, steps, done = 0, 0, False\n",
    "        trajectory = [env.agent_pos]\n",
    "        fell_in_trap = False\n",
    "\n",
    "        while not done:\n",
    "            action = agent.get_action(state, training=True)\n",
    "            next_state, reward, done, _, info = env.step(action)\n",
    "            fell_in_trap = info.get(\"fell_in_trap\", False)\n",
    "            agent.update(state, action, reward, next_state, done)\n",
    "            trajectory.append(env.agent_pos)\n",
    "            total_reward += reward\n",
    "            state = next_state\n",
    "            steps += 1\n",
    "            if steps > 500:\n",
    "                done = True\n",
    "\n",
    "        agent.decay_epsilon()\n",
    "        agent.training_info['episodes'].append(episode + 1)\n",
    "        agent.training_info['rewards'].append(total_reward)\n",
    "        agent.training_info['steps'].append(steps)\n",
    "        agent.training_info['epsilons'].append(agent.epsilon)\n",
    "        agent.training_info['avg_q_values'].append(np.mean(agent.q_table))\n",
    "\n",
    "        success = (env.agent_pos == env.goal_pos)\n",
    "\n",
    "        if visualize and visualizer and (episode + 1) % show_every == 0:\n",
    "            visualizer.add_episode_data(total_reward, steps)\n",
    "            if not visualizer.draw_episode(\n",
    "                    trajectory, episode + 1, num_episodes,\n",
    "                    steps, total_reward, agent.epsilon, success, fell_in_trap):\n",
    "                print(\"Training interrupted\")\n",
    "                visualizer.close()\n",
    "                return\n",
    "\n",
    "    if visualizer:\n",
    "        time.sleep(1)\n",
    "        visualizer.close()\n",
    "    print(\"=\" * 60)\n",
    "    print(\"TRAINING COMPLETED!\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------------\n",
    "# MAIN\n",
    "# ---------------------------------------------------------------\n",
    "def main():\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"  MAZE RUNNER: Q-LEARNING PROJECT\")\n",
    "    print(\"=\"*70)\n",
    "    print(\"\\n Units: MDP | Bellman | Q-Learning | Tabular Methods\")\n",
    "    print(\"=\"*70 + \"\\n\")\n",
    "\n",
    "    MAZE_SIZE = 10\n",
    "    NUM_EPISODES = 500\n",
    "\n",
    "    print(\"Creating Environment...\")\n",
    "    env = MazeEnv(maze_size=MAZE_SIZE)\n",
    "\n",
    "    print(\"Initializing Q-Learning Agent...\")\n",
    "    agent = QLearningAgent(\n",
    "        state_size=env.observation_space.n,\n",
    "        action_size=env.action_space.n,\n",
    "        learning_rate=0.1,\n",
    "        discount_factor=0.99,\n",
    "        epsilon=1.0,\n",
    "        epsilon_decay=0.995,\n",
    "        epsilon_min=0.01\n",
    "    )\n",
    "\n",
    "    print(\"Starting Training...\\n\")\n",
    "    input(\"Press ENTER to start...\")\n",
    "\n",
    "    try:\n",
    "        train_agent(env, agent, num_episodes=NUM_EPISODES, visualize=True, show_every=1)\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"\\nInterrupted\")\n",
    "\n",
    "    print(\"\\nPROJECT COMPLETED!\\n\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        main()\n",
    "    except Exception as e:\n",
    "        print(f\"\\n Error: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "    finally:\n",
    "        pygame.quit()\n",
    "        print(\"\\n Done!\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nven",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
